{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ccccd348",
   "metadata": {},
   "source": [
    "1. Download dependencies and hg38 genome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff424dfd-9e5f-4149-be4d-3fc90670f7a6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pyfaidx in ./.local/lib/python3.10/site-packages (0.8.1.3)\n",
      "Requirement already satisfied: transformers in ./.local/lib/python3.10/site-packages (4.51.3)\n",
      "Requirement already satisfied: datasets in ./.local/lib/python3.10/site-packages (3.5.0)\n",
      "Requirement already satisfied: tqdm in ./.local/lib/python3.10/site-packages (4.67.1)\n",
      "Requirement already satisfied: importlib-metadata in /usr/lib/python3/dist-packages (from pyfaidx) (4.6.4)\n",
      "Requirement already satisfied: packaging in /usr/lib/python3/dist-packages (from pyfaidx) (21.3)\n",
      "Requirement already satisfied: requests in ./.local/lib/python3.10/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/lib/python3/dist-packages (from transformers) (1.21.5)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in ./.local/lib/python3.10/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in ./.local/lib/python3.10/site-packages (from transformers) (0.30.2)\n",
      "Requirement already satisfied: filelock in /usr/lib/python3/dist-packages (from transformers) (3.6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./.local/lib/python3.10/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/lib/python3/dist-packages (from transformers) (5.4.1)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in ./.local/lib/python3.10/site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: fsspec[http]<=2024.12.0,>=2023.1.0 in /usr/lib/python3/dist-packages (from datasets) (2024.3.1)\n",
      "Requirement already satisfied: pandas in /usr/lib/python3/dist-packages (from datasets) (1.3.5)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in ./.local/lib/python3.10/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: aiohttp in ./.local/lib/python3.10/site-packages (from datasets) (3.11.18)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in ./.local/lib/python3.10/site-packages (from datasets) (20.0.0)\n",
      "Requirement already satisfied: xxhash in ./.local/lib/python3.10/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in ./.local/lib/python3.10/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./.local/lib/python3.10/site-packages (from aiohttp->datasets) (1.6.0)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in ./.local/lib/python3.10/site-packages (from aiohttp->datasets) (5.0.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in ./.local/lib/python3.10/site-packages (from aiohttp->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./.local/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.2)\n",
      "Requirement already satisfied: propcache>=0.2.0 in ./.local/lib/python3.10/site-packages (from aiohttp->datasets) (0.3.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./.local/lib/python3.10/site-packages (from aiohttp->datasets) (6.4.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/lib/python3/dist-packages (from aiohttp->datasets) (21.2.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in ./.local/lib/python3.10/site-packages (from aiohttp->datasets) (1.20.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/lib/python3/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.10.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.local/lib/python3.10/site-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->transformers) (2020.6.20)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/lib/python3/dist-packages (from requests->transformers) (1.26.5)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->transformers) (3.3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-28 03:32:38.565804: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1745811158.588499  150437 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1745811158.595557  150437 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "!pip install pyfaidx transformers datasets tqdm ipywidgets\n",
    "!wget -c http://hgdownload.cse.ucsc.edu/goldenpath/hg38/bigZips/hg38.fa.gz\n",
    "!gunzip -k hg38.fa.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59263612",
   "metadata": {},
   "source": [
    "2. Generate input sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af640a0-219c-43fc-9709-d3c320931326",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "from pyfaidx import Fasta\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import torch\n",
    "\n",
    "df = pd.read_csv(\"all.csv\")\n",
    "\n",
    "# Load reference genome\n",
    "genome = Fasta(\"hg38.fa\")\n",
    "\n",
    "# Set window size\n",
    "window = 512\n",
    "\n",
    "# Extract 512bp flanking sequences (total 1024+1bp)\n",
    "\n",
    "def get_sequence(row, flank_size=256):\n",
    "    try:\n",
    "        chrom = str(row[\"#CHROM\"])\n",
    "        if not chrom.startswith(\"chr\"):\n",
    "            chrom = \"chr\" + chrom\n",
    "\n",
    "        pos = int(row[\"POS\"])\n",
    "        start = max(0, pos - flank_size - 1)\n",
    "        end = pos + flank_size\n",
    "\n",
    "        # Check if chromosome is present in the genome\n",
    "        if chrom not in genome:\n",
    "            return None\n",
    "\n",
    "        seq = genome[chrom][start:end].seq.upper()\n",
    "\n",
    "        # Validate length\n",
    "        if len(seq) != (2 * flank_size + 1):\n",
    "            return None\n",
    "\n",
    "        return seq\n",
    "    except Exception as e:\n",
    "        print(f\"[⚠️ get_sequence] Error: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def generate_mutant_sequence(row, flank_size=256):\n",
    "    try:\n",
    "        seq = list(row[\"Context_Sequence\"])\n",
    "        mut_pos = flank_size  # The mutation position is in the center\n",
    "\n",
    "        # Ensure REF/ALT are single nucleotides\n",
    "        if len(row[\"REF\"]) != 1 or len(row[\"ALT\"]) != 1:\n",
    "            return None\n",
    "\n",
    "        # Ensure REF matches the reference sequence\n",
    "        if seq[mut_pos] != row[\"REF\"]:\n",
    "            return None\n",
    "\n",
    "        seq[mut_pos] = row[\"ALT\"]\n",
    "        return \"\".join(seq)\n",
    "    except Exception as e:\n",
    "        print(f\"[⚠️ generate_mutant_sequence] Error: {e}\")\n",
    "        return None\n",
    "\n",
    "# Extract context sequences (reference)\n",
    "tqdm.pandas()\n",
    "df[\"Context_Sequence\"] = df.progress_apply(lambda row: get_sequence(row, flank_size=window), axis=1)\n",
    "df.dropna(subset=[\"Context_Sequence\"], inplace=True)\n",
    "\n",
    "# Generate mutant sequences\n",
    "df[\"Mutant_Sequence\"] = df.progress_apply(lambda row: generate_mutant_sequence(row, flank_size=window), axis=1)\n",
    "df.dropna(subset=[\"Mutant_Sequence\"], inplace=True)\n",
    "\n",
    "print(f\"Successfully generated context and mutant sequences. Total valid records: {len(df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9778fde",
   "metadata": {},
   "source": [
    "3. Compute the DNABERT2 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f104ff7a-d470-41ce-a85e-972334e81db1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertModel were not initialized from the model checkpoint at zhihan1996/DNABERT-2-117M and are newly initialized: ['embeddings.position_embeddings.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.9.output.dense.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting wild-type sequences...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting Batch: 100%|██████████| 1/1 [00:00<00:00,  2.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting mutant sequences...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting Batch: 100%|██████████| 1/1 [00:00<00:00,  2.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DNABERT2 inference complete. L2 distance scores have been added to the dataset as 'DB2'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "from transformers import BertModel, AutoTokenizer\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "df = df.head(100)\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load DNABERT2 model and tokenizer\n",
    "model = BertModel.from_pretrained(\"zhihan1996/DNABERT-2-117M\", trust_remote_code=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"zhihan1996/DNABERT-2-117M\", trust_remote_code=True)\n",
    "\n",
    "# Enable multi-GPU if available\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(f\"Using {torch.cuda.device_count()} GPUs for inference (DataParallel)\")\n",
    "    model = torch.nn.DataParallel(model)\n",
    "\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Prediction function with batching & multi-GPU support\n",
    "@torch.no_grad()\n",
    "def predict_batch(sequences, batch_size=32):\n",
    "    embeddings = []\n",
    "    dataloader = DataLoader(sequences, batch_size=batch_size)\n",
    "    for batch in tqdm(dataloader, desc=\"Predicting Batch\", leave=True):\n",
    "        inputs = tokenizer(list(batch), return_tensors=\"pt\", padding=True, truncation=True, max_length=4096)\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        outputs = model(**inputs)\n",
    "        cls_embeddings = outputs.last_hidden_state[:, 0, :].cpu()\n",
    "        embeddings.append(cls_embeddings)\n",
    "    return torch.cat(embeddings, dim=0).numpy()\n",
    "\n",
    "# Extract wild-type and mutant sequences\n",
    "context_sequences = df[\"Context_Sequence\"].tolist()\n",
    "mutant_sequences = df[\"Mutant_Sequence\"].tolist()\n",
    "\n",
    "# Run inference\n",
    "print(\"Predicting wild-type sequences...\")\n",
    "wt_embeddings = predict_batch(context_sequences, batch_size=512)\n",
    "\n",
    "print(\"Predicting mutant sequences...\")\n",
    "mut_embeddings = predict_batch(mutant_sequences, batch_size=512)\n",
    "\n",
    "# Compute L2 distance between wild-type and mutant embeddings\n",
    "l2_scores = np.linalg.norm(wt_embeddings - mut_embeddings, axis=1)\n",
    "\n",
    "# Save L2 scores into the original dataframe\n",
    "df[\"DB2\"] = l2_scores\n",
    "\n",
    "# (Optional) Save result if needed\n",
    "df.to_csv(\"db2.csv\", index=False)\n",
    "\n",
    "print(\"DNABERT2 inference complete. L2 distance scores have been added to the dataset as 'DB2'.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
